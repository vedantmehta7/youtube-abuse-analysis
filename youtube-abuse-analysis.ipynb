{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Scaled Abuse Analysis Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from google.cloud import videointelligence\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection and preprocessing\n",
    "Data Collection: Automates the process of gathering video data from YouTube, which can be used for analysis or machine learning projects.\n",
    "<br>\n",
    "Simulated Data: Adds extra columns to simulate real-world scenarios where videos might violate policies or receive user reports, useful for testing algorithms or models.\n",
    "\n",
    "<ol> Construct a URL to call the YouTube API with your API Key </ol>\n",
    "<ol> send the request to get the video Data</ol>\n",
    "<ol> parse the response to video details like id, title, description, and publishedAt</ol>\n",
    "<ol> store the data in list of dictionaries</ol>\n",
    "<ol> Convert the list of video data into a Pandas DataFrame for easy manipulation and analysis.</ol>\n",
    "<ol> Violation Type: Randomly assigns a type of policy violation (e.g., 'spam', 'hate speech') to each video. </ol>\n",
    "<ol> User Reports: Randomly generates a number of user reports for each video.</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_video_data(api_key, max_results=1000):\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/search?key={api_key}&part=snippet&type=video&maxResults={max_results}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    videos = []\n",
    "    for item in data.get('items', []):\n",
    "        video = {\n",
    "            'id': item['id']['videoId'],\n",
    "            'title': item['snippet']['title'],\n",
    "            'description': item['snippet']['description'],\n",
    "            'publishedAt': item['snippet']['publishedAt']\n",
    "        }\n",
    "        videos.append(video)\n",
    "    return pd.DataFrame(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating data collection\n",
    "video_data = collect_video_data('AIzaSyDhHFhatbX3mUMu98yDfmHs43z0cbvH1_o')\n",
    "\n",
    "# Add simulated policy violation data\n",
    "video_data['violation_type'] = np.random.choice(['none', 'spam', 'hate_speech', 'nudity', 'violence'], size=len(video_data))\n",
    "video_data['user_reports'] = np.random.randint(0, 100, size=len(video_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>violation_type</th>\n",
       "      <th>user_reports</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hbivC9ztGOE</td>\n",
       "      <td>üé• EXAM (2009) | Full Movie Trailer in HD | 1080p</td>\n",
       "      <td>Eight candidates for a highly desirable corpor...</td>\n",
       "      <td>2018-06-02T12:00:03Z</td>\n",
       "      <td>none</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VCqwS05SU4s</td>\n",
       "      <td>Millet Noodles üçú #proteinrichrecipe #proteinno...</td>\n",
       "      <td></td>\n",
       "      <td>2024-09-10T05:07:45Z</td>\n",
       "      <td>none</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cn27hLC0X90</td>\n",
       "      <td>Trump-Tulsi Vs Kamala Harris LIVE | Trump-Harr...</td>\n",
       "      <td>Trump-Tulsi Vs Kamala Harris LIVE | Trump-Harr...</td>\n",
       "      <td>2024-09-04T20:48:54Z</td>\n",
       "      <td>hate_speech</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRxKwivxmls</td>\n",
       "      <td>Use expiring milk to make Korean rice mask ü•õ</td>\n",
       "      <td></td>\n",
       "      <td>2024-09-10T00:55:15Z</td>\n",
       "      <td>nudity</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fd56fk3bMAw</td>\n",
       "      <td>Knock-knock, What a car is at the door? Best S...</td>\n",
       "      <td>Knock-knock, What a car is at the door? Best S...</td>\n",
       "      <td>2024-04-17T17:18:02Z</td>\n",
       "      <td>nudity</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "0  hbivC9ztGOE   üé• EXAM (2009) | Full Movie Trailer in HD | 1080p   \n",
       "1  VCqwS05SU4s  Millet Noodles üçú #proteinrichrecipe #proteinno...   \n",
       "2  cn27hLC0X90  Trump-Tulsi Vs Kamala Harris LIVE | Trump-Harr...   \n",
       "3  TRxKwivxmls       Use expiring milk to make Korean rice mask ü•õ   \n",
       "4  Fd56fk3bMAw  Knock-knock, What a car is at the door? Best S...   \n",
       "\n",
       "                                         description           publishedAt  \\\n",
       "0  Eight candidates for a highly desirable corpor...  2018-06-02T12:00:03Z   \n",
       "1                                                     2024-09-10T05:07:45Z   \n",
       "2  Trump-Tulsi Vs Kamala Harris LIVE | Trump-Harr...  2024-09-04T20:48:54Z   \n",
       "3                                                     2024-09-10T00:55:15Z   \n",
       "4  Knock-knock, What a car is at the door? Best S...  2024-04-17T17:18:02Z   \n",
       "\n",
       "  violation_type  user_reports  \n",
       "0           none            22  \n",
       "1           none            14  \n",
       "2    hate_speech            60  \n",
       "3         nudity            28  \n",
       "4         nudity            48  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x='violation_type', data=df) # to understand which type of violation type is most common\n",
    "    plt.title('Distribution of Policy Violations')\n",
    "    plt.savefig('violation_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "    df['violation_type_encoded'] = df['violation_type'].astype('category').cat.codes\n",
    "    correlation = df[['user_reports', 'violation_type_encoded']].corr() #shows the correlation between user_reports and violation_types\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "    plt.title('Correlation between User Reports and Violations')\n",
    "    plt.savefig('correlation_heatmap.png')\n",
    "    plt.close()\n",
    "\n",
    "perform_eda(video_data)\n",
    "# -0.011 meaning: changes in one variable have little to no effect on the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(df):\n",
    "    tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "    text_features = tfidf.fit_transform(df['description'])\n",
    "    return pd.DataFrame(text_features.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "def extract_visual_features(video_id):\n",
    "    # Simulating visual feature extraction\n",
    "    # In a real scenario, you would use the Video Intelligence API\n",
    "    return np.random.rand(1, 100)  # 100 visual features\n",
    "\n",
    "text_features = extract_text_features(video_data)\n",
    "visual_features = pd.DataFrame(np.random.rand(len(video_data), 100), columns=[f'visual_feature_{i}' for i in range(100)])\n",
    "\n",
    "# Combine all features\n",
    "X = pd.concat([video_data[['user_reports']], text_features, visual_features], axis=1)\n",
    "y = video_data['violation_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline for preprocessing and model training\n",
    "numeric_features = ['user_reports']\n",
    "text_features = text_features.columns\n",
    "visual_features = visual_features.columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('text', 'passthrough', text_features),\n",
    "        ('visual', 'passthrough', visual_features)\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.3s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=10, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=20, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=2, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=30, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=2, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=2, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=100; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=200; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.2s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.1s\n",
      "[CV] END classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=300; total time=   0.1s\n",
      "Best parameters: {'classifier__max_depth': 10, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 100}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " hate_speech       0.50      0.33      0.40         3\n",
      "        none       0.00      0.00      0.00         4\n",
      "      nudity       0.12      1.00      0.22         1\n",
      "        spam       0.00      0.00      0.00         1\n",
      "    violence       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.20        10\n",
      "   macro avg       0.12      0.27      0.12        10\n",
      "weighted avg       0.16      0.20      0.14        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [10, 20, 30, None],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalability and Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "dask_df = dd.from_pandas(X, npartitions=4)\n",
    "dask_predictions = dask_df.map_partitions(best_model.predict).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incident Response System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of potential incidents detected: 0\n"
     ]
    }
   ],
   "source": [
    "def detect_anomalies(df, threshold=2):\n",
    "    mean = df['user_reports'].mean()\n",
    "    std = df['user_reports'].std()\n",
    "    df['is_anomaly'] = df['user_reports'] > (mean + threshold * std)\n",
    "    return df\n",
    "\n",
    "anomalies = detect_anomalies(video_data)\n",
    "print(f\"Number of potential incidents detected: {anomalies['is_anomaly'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.84\n",
      "Project execution completed. Check the generated visualizations for insights.\n"
     ]
    }
   ],
   "source": [
    "def generate_report(df, predictions):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x=predictions)\n",
    "    plt.title('Distribution of Predicted Policy Violations')\n",
    "    plt.savefig('predicted_violations.png')\n",
    "    plt.close()\n",
    "\n",
    "    accuracy = (predictions == df['violation_type']).mean()\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    cm = confusion_matrix(df['violation_type'], predictions)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix of Policy Violations')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "generate_report(video_data, dask_predictions)\n",
    "\n",
    "print(\"Project execution completed. Check the generated visualizations for insights.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
